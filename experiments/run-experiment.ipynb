{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root directory setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append the absolute path of the **root directory** to sys to access other subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamborgula/dev/bachelor_thesis/federated-learning\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "print(nb_dir)\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load arguments\n",
    "- for every experiments, there are some arguments which are different from the default configuration\n",
    "- replace the name of the experiment folder below to load the correct experiment args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"mag-p06-spar06-comp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/adamborgula/dev/bachelor_thesis/federated-learning/save/results/mag/mag-p06-spar06-comp/args.txt\n"
     ]
    }
   ],
   "source": [
    "from utils.options import args_parser\n",
    "\n",
    "# Read the arguments from the text file\n",
    "args_file = nb_dir + '/save/results/mag/' + experiment + '/args.txt'\n",
    "print(args_file)\n",
    "with open(args_file, 'r') as file:\n",
    "    args = args_parser(file.read().split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "import random\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, cifar_noniid\n",
    "from models.Update import LocalUpdate\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar\n",
    "from models.Fed import FedAvg, GlobalAvg\n",
    "from models.test import test_img\n",
    "\n",
    "from utils.prune_parameters import *\n",
    "from numpy import random\n",
    "\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_successful_users` selects p users randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 9]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "def get_successful_users(p, num_users):\n",
    "    group1 = []\n",
    "    group2 = []\n",
    "    for i in range(0, num_users//2):\n",
    "        if random.random() <= p:\n",
    "            group1.append(i)\n",
    "    for i in range(num_users//2, num_users):\n",
    "        if random.random() <= p:\n",
    "            group2.append(i)\n",
    "    if len(group1) == 0:\n",
    "        user = np.random.choice(range(num_users//2), 1, replace=False)\n",
    "        group1.append(user[0])\n",
    "    if len(group2) == 0:\n",
    "        user = np.random.choice(range(num_users//2, num_users), 1, replace=False)\n",
    "        group2.append(user[0])\n",
    "    return group1 + group2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity based compensation\n",
    "- `similarity_score` computes the similarty score between clients **u** and **v**\n",
    "- `similarity_based_compensation` computes the similarity between all pairs of clients and returns the **similarity_matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(u: int,v: int, w_locals, args) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the similarity score between two clients based on the Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        u (int): Index of the first client.\n",
    "        v (int): Index of the second client.\n",
    "        w_locals: The weights of each client\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score between the two clients based on the Euclidean distance.\n",
    "    \"\"\"\n",
    "    w_u, w_v = w_locals[u], w_locals[v] \n",
    "    diference_vector = torch.tensor([]).to(args.device)\n",
    "    for k in w_u.keys():\n",
    "        diference_vector = torch.cat((diference_vector, torch.flatten(w_u[k]) - torch.flatten(w_v[k])), dim=0)\n",
    "    distance = torch.norm(diference_vector)\n",
    "    return -distance\n",
    "\n",
    "def similarity_based_compensation(w_locals, users_received, args, sm):\n",
    "    similarity_matrix = [i for i in range(len(w_locals))]\n",
    "    for u in range(args.num_users):\n",
    "        if u in users_received:\n",
    "            continue\n",
    "\n",
    "        # find the most similar received / updated user\n",
    "        most_similar = users_received[0]\n",
    "        for neighbor in users_received:\n",
    "            \n",
    "            similarity = similarity_score(u, neighbor, w_locals, args)\n",
    "            sm[u][neighbor] = similarity.item() * -1\n",
    "            sm[neighbor][u] = similarity.item() * -1\n",
    "            if similarity > similarity_score(u, most_similar, w_locals, args):\n",
    "                most_similar = neighbor\n",
    "\n",
    "        # update the similarity matrix\n",
    "        similarity_matrix[u] = most_similar\n",
    "    print(similarity_matrix)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # parse args\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "    iters = 5\n",
    "    alphas = [i/5 for i in range(0, iters)]\n",
    "    # seeds = [0,99,345]\n",
    "    seeds = [0]\n",
    "    x_vals = [i for i in range(args.epochs_start, args.epochs_end, args.epochs_step)]\n",
    "    y_vals = {'acc': [], 'loss': []}\n",
    "    sm = [[0 for _ in range(args.num_users)] for _ in range(args.num_users)]\n",
    "\n",
    "    for seed in seeds:\n",
    "        # set random seed\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "        # load dataset and split users\n",
    "        if args.dataset == 'mnist':\n",
    "            trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "            dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "            dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "            # sample users\n",
    "            if args.iid:\n",
    "                dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "            else:\n",
    "                dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "        elif args.dataset == 'cifar':\n",
    "            trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "            dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "            dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "            if args.iid:\n",
    "                dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "            else:\n",
    "                dict_users = cifar_noniid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            exit('Error: unrecognized dataset')\n",
    "        img_size = dataset_train[0][0].shape\n",
    "\n",
    "        # build model\n",
    "        if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "            net_glob = CNNCifar(args=args).to(args.device)\n",
    "            #model = models.vgg16(weights = None)\n",
    "\n",
    "            # Step 4: Modify last layer\n",
    "            #num_classes = 10  # CIFAR-10 has 10 classes\n",
    "            #model.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "            #net_glob = model.to(args.device)\n",
    "        elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "            net_glob = CNNMnist(args=args).to(args.device)\n",
    "        elif args.model == 'mlp':\n",
    "            len_in = 1\n",
    "            for x in img_size:\n",
    "                len_in *= x\n",
    "            net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "        elif args.model == 'resnet18' and args.dataset == 'cifar':\n",
    "            model = models.resnet18()\n",
    "            # Modify the last layer to have 10 output classes (CIFAR-10 has 10 classes)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, 10)\n",
    "            net_glob = model.to(args.device)\n",
    "        else:\n",
    "            exit('Error: unrecognized model')\n",
    "        print(net_glob)\n",
    "        net_glob.train()\n",
    "\n",
    "        # copy weights\n",
    "        w_glob = net_glob.state_dict()\n",
    "\n",
    "        # training\n",
    "        loss_train = []\n",
    "        cv_loss, cv_acc = [], []\n",
    "        val_loss_pre, counter = 0, 0\n",
    "        net_best = None\n",
    "        best_loss = None\n",
    "        val_acc_list, net_list = [], []\n",
    "        masks = [randomMask(net_glob, args.device, args.compression) for _ in range(args.num_users)]\n",
    "        print('mask', len(masks), len(masks[0]), type(masks[0]), masks[0][0].size()) # Shape = 100 x 6 x 400 x 3072\n",
    "        w_locals = [copy.deepcopy(w_glob) for i in range(args.num_users)]\n",
    "\n",
    "        for iter in range(args.epochs_end):\n",
    "            loss_locals = []\n",
    "            idxs_users = get_successful_users(args.p, args.num_users) \n",
    "            print('RANDOM USER INDICES', idxs_users)\n",
    "            for idx in idxs_users:\n",
    "                local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                w, loss, mask = local.train(net=copy.deepcopy(net_glob).to(args.device), mask=masks[idx], train_iter=iter)\n",
    "                # print(w.keys())\n",
    "                masks[idx] = mask\n",
    "                w_locals[idx] = copy.deepcopy(w)\n",
    "                loss_locals.append(copy.deepcopy(loss))\n",
    "            # update global weights\n",
    "            # similarity logic\n",
    "            if args.compensation:\n",
    "                similarity_matrix = similarity_based_compensation(w_locals, users_received=idxs_users, args=args, sm = sm)\n",
    "                w_glob = GlobalAvg(w_locals, similarity_matrix)\n",
    "            else:\n",
    "                w_glob = GlobalAvg(w_locals, idxs_users)\n",
    "\n",
    "            # copy weight to net_glob\n",
    "            net_glob.load_state_dict(w_glob)\n",
    "\n",
    "            # print loss\n",
    "            loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "            print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "            loss_train.append(loss_avg)\n",
    "            if (iter - args.epochs_start) % args.epochs_step == 0 and iter >= args.epochs_start:\n",
    "                net_glob.eval()\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "                print(\"Testing loss: {:.2f}\".format(loss_test))\n",
    "\n",
    "                y_vals['acc'].append(acc_test.item())\n",
    "                y_vals['loss'].append(loss_test)\n",
    "                #y_vals[args.pruner].append(0)\n",
    "\n",
    "    print('test accuracy: ', y_vals['acc'])\n",
    "    print('test loss: ', y_vals['loss'])\n",
    "    # Plot both charts on the same axis\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals['acc'])\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Top-1 Accuracy')\n",
    "    plt.title('')\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    date = now.strftime(\"%Y_%m_%d\")\n",
    "    time = now.strftime(\"%H_%M_%S\")\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    save_dir = f\"../save/{date}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"{save_dir}/similarity_test_acc_{args.prune_epochs}_{args.compensation}_{args.dataset}_{args.model}_{args.iid}_{args.p}_{args.num_users}_{args.epochs_start}_{args.epochs_end}_{time}.png\")\n",
    "    # plt.savefig('../save/synflow_test_{}_{}_{}_{}_{}_{}_{}.png'.format(args.prune_epochs, args.dataset, args.model, args.iid, args.frac, args.num_users, args.epochs))\n",
    "    plt.figure()\n",
    "    plt.plot(x_vals, y_vals['loss'])\n",
    "    plt.xlabel('Communication Rounds')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('')\n",
    "    plt.savefig(f\"{save_dir}/similarity_test_loss_{args.prune_epochs}_{args.compensation}_{args.dataset}_{args.model}_{args.iid}_{args.p}_{args.num_users}_{args.epochs_start}_{args.epochs_end}_{time}.png\")\n",
    "    np.savez(f\"{save_dir}/similarity_test_loss_{args.prune_epochs}_{args.compensation}_{args.dataset}_{args.model}_{args.iid}_{args.p}_{args.num_users}_{args.epochs_start}_{args.epochs_end}_{time}\", y = np.array(y_vals['loss']), x = np.array(x_vals))\n",
    "    np.savez(f\"{save_dir}/similarity_test_acc_{args.prune_epochs}_{args.compensation}_{args.dataset}_{args.model}_{args.iid}_{args.p}_{args.num_users}_{args.epochs_start}_{args.epochs_end}_{time}\", y = np.array(y_vals['acc']), x = np.array(x_vals))\n",
    "    plt.imshow(np.array(sm), cmap='viridis')  # cmap specifies the colormap (color scheme)\n",
    "    plt.colorbar() \n",
    "    plt.savefig(f\"{save_dir}/sm_{args.prune_epochs}_{args.compensation}_{args.dataset}_{args.model}_{args.iid}_{args.p}_{args.num_users}_{args.epochs_start}_{args.epochs_end}_{time}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment():\n",
    "    x_vals, y_vals = [],[]\n",
    "\n",
    "    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "    alphas = [i/5 for i in range(10, 10+args.iters)]\n",
    "    seeds = [0]\n",
    "    x_vals = [10**alpha for alpha in alphas]\n",
    "    y_vals = {'mag': [], 'synflow': []}\n",
    "\n",
    "    for c in x_vals:\n",
    "        for seed in seeds:\n",
    "            np.random.seed(args.seed)\n",
    "            torch.manual_seed(args.seed)\n",
    "            torch.cuda.manual_seed(args.seed)\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            for pruner in ('synflow', 'mag'):\n",
    "                args.pruner = pruner\n",
    "                args.compression = c\n",
    "\n",
    "                # load dataset and split users\n",
    "                if args.dataset == 'mnist':\n",
    "                    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "                    dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "                    dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "                    # sample users\n",
    "                    if args.iid:\n",
    "                        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "                    else:\n",
    "                        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "                elif args.dataset == 'cifar':\n",
    "                    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "                    dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "                    dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "                    if args.iid:\n",
    "                        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "                    else:\n",
    "                        exit('Error: only consider IID setting in CIFAR10')\n",
    "                else:\n",
    "                    exit('Error: unrecognized dataset')\n",
    "                img_size = dataset_train[0][0].shape\n",
    "\n",
    "                # build model\n",
    "                if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "                    net_glob = CNNCifar(args=args).to(args.device)\n",
    "                    #model = models.vgg16(weights = None)\n",
    "\n",
    "                    # Step 4: Modify last layer\n",
    "                    #num_classes = 10  # CIFAR-10 has 10 classes\n",
    "                    #model.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "                    #net_glob = model.to(args.device)\n",
    "                elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "                    net_glob = CNNMnist(args=args).to(args.device)\n",
    "                elif args.model == 'mlp':\n",
    "                    len_in = 1\n",
    "                    for x in img_size:\n",
    "                        len_in *= x\n",
    "                    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "                else:\n",
    "                    exit('Error: unrecognized model')\n",
    "                print(net_glob)\n",
    "                net_glob.train()\n",
    "\n",
    "                # copy weights\n",
    "                w_glob = net_glob.state_dict()\n",
    "\n",
    "                # training\n",
    "                loss_train = []\n",
    "                cv_loss, cv_acc = [], []\n",
    "                val_loss_pre, counter = 0, 0\n",
    "                net_best = None\n",
    "                best_loss = None\n",
    "                val_acc_list, net_list = [], []\n",
    "\n",
    "                if args.all_clients: \n",
    "                    print(\"Aggregation over all clients\")\n",
    "                    w_locals = [w_glob for i in range(args.num_users)]\n",
    "                for iter in range(args.epochs):\n",
    "                    loss_locals = []\n",
    "                    if not args.all_clients:\n",
    "                        w_locals = []\n",
    "                    m = max(int(args.frac * args.num_users), 1)\n",
    "                    idxs_users = [0]\n",
    "                    for idx in idxs_users:\n",
    "                        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "                        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "                        if args.all_clients:\n",
    "                            w_locals[idx] = copy.deepcopy(w)\n",
    "                        else:\n",
    "                            w_locals.append(copy.deepcopy(w))\n",
    "                        loss_locals.append(copy.deepcopy(loss))\n",
    "                    # update global weights\n",
    "                    w_glob = FedAvg(w_locals)\n",
    "\n",
    "                    # copy weight to net_glob\n",
    "                    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "                    # print loss\n",
    "                    loss_avg = sum(loss_locals) / len(loss_locals)\n",
    "                    print('Round {:3d}, Average loss {:.3f}'.format(iter, loss_avg))\n",
    "                    loss_train.append(loss_avg)\n",
    "\n",
    "                # plot loss curve\n",
    "                # plt.figure()\n",
    "                # plt.plot(range(len(loss_train)), loss_train)\n",
    "                # plt.ylabel('train_loss')\n",
    "                #   plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "                # testing\n",
    "                net_glob.eval()\n",
    "                acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "                acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "                print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "                print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
    "\n",
    "                y_vals[args.pruner].append(acc_test)\n",
    "                #y_vals[args.pruner].append(0)\n",
    "\n",
    "    print('synflow test accuracy: ', y_vals['synflow'])\n",
    "    print('mag test accuracy: ', y_vals['mag'])\n",
    "    # Plot both charts on the same axis\n",
    "    plt.figure()\n",
    "    plt.xscale('log')\n",
    "    plt.plot(x_vals, y_vals['synflow'], label='Synflow', linestyle='-', marker='o', color='r')\n",
    "    plt.plot(x_vals, y_vals['mag'], label='Mag', linestyle='-', marker='o', color='b')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.title('Synflow vs Mag')\n",
    "\n",
    "    # Add legend\n",
    "    plt.legend()\n",
    "\n",
    "   # Show plot\n",
    "    if args.show:\n",
    "        plt.show()\n",
    "\n",
    "    # Save plot\n",
    "    if args.save:\n",
    "        plt.savefig('../save/synflow_test_{}_{}_{}.png'.format(args.prune_epochs, args.dataset, args.model))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
